### Hi, welcome to my Github ðŸ‘‹

<!-- ![visitors](https://visitor-badge.glitch.me/badge?page_id=xiao9905.xiao9905&left_color=green&right_color=red) -->

I am Xiao Liu, a second-year master CS student in Tsinghua University.

- ðŸ”­ Interested in Machine Learning, Data Mining, NLP and Knowledge Graph.
- ðŸŒ± Find my up-to-date publication list in [**Google Scholar**](https://scholar.google.com/citations?user=VKI8EhUAAAAJ)! Some of my proud leading works:
  
  <details><summary><b>Large Language Model (LLM) Pre-training and Transfer Learning</b></summary>
  
  * [P-tuning](https://github.com/THUDM/P-tuning) and [P-tuning v2 (ACL'22)](https://github.com/THUDM/P-tuning-v2): pioneer works on ***prompt tuning***
  * [GLM-130B (ICLR'23)](https://github.com/THUDM/GLM-130B): probably the best open-sourced LLM so far; an open bilingual (Enligsh & Chinese) pre-trained model with 130 billion parameters based on [GLM (ACL'22)](https://github.com/THUDM/GLM); better than GPT-3 175B on LAMBADA and MMLU.
  * [ChatGLM-6B](https://github.com/THUDM) & [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B): an open bilingual dialogue language model that requires only 6GB to run. Receiving [![GitHub stars](https://badgen.net/github/stars/THUDM/ChatGLM-6B)](https://GitHub.com/THUDM/ChatGLM-6B/stargazers/).
  * [WebGLM (KDD'23)](https://github.com/THUDM/WebGLM): an efficient web-enhanced question answering system based on [GLM-10B](https://github.com/THUDM/GLM), outperforming WebGPT-13B and approaching WebGPT-175B performance in human evaluation.
  </details>
  
  <details><summary><b>Evaluation and Assessment of Generative Models</b></summary>
  
  * [ImageReward](https://github.com/THUDM/ImageReward): the first general-purpose text-to-image human preference reward model (RM) for RLHF, outperforming CLIP/BLIP/Aesthetic by 30% in terms of human preference prediction.
  
  </details>
  
  <details><summary><b>Knowledge Graph Construction and Reasoning</b></summary>
  
  * [SelfKG (WWW'22)](https://github.com/THUDM/SelfKG): self-supervised alignment can be comparable to supervised ones, ***Best Paper Nominee*** in WWW 2022.
  * [kgTransformer (KDD'22)](https://github.com/THUDM/kgTransformer): pre-training knowledge graph transformers for complex logical reasoning
  
  </details>
  
  <details><summary><b>Self-supervised Learning and its Applications</b></summary>
  
  * [Self-supervised Learning: Generative or Contrastive (TKDE'21)](https://arxiv.org/pdf/2006.08218.pdf): one of the most cited survey on self-supervised learning
  
  </details>
- ðŸ¤” Dedicated to building web-scale knowledge systems via both [Large Pre-trained Model](https://github.com/THUDM/GLM-130B) and [Symbolic Graph Reasoning](https://github.com/THUDM/kgTransformer).
- ðŸ’¬ Feel free to drop me an email for:
  * Any form of collaboration
  * Any issue about my works or code
  * Interesting ideas to discuss or just chatting
