### Hi, welcome to my Github ðŸ‘‹

<!-- ![visitors](https://visitor-badge.glitch.me/badge?page_id=xiao9905.xiao9905&left_color=green&right_color=red) -->

I am Xiao Liu, a fourth-year PhD student in Tsinghua University since 2021.

- ðŸ”­ Interested in Machine Learning, Natural Language Processing, and Foundation Models.
- ðŸŒ± Find my up-to-date publication list in [**Google Scholar**](https://scholar.google.com/citations?user=VKI8EhUAAAAJ)! Some of my proud leading works:
  
  <details><summary><b>Large Language Model (LLM) Training and Prompt Learning</b></summary>
  
  * [P-tuning](https://github.com/THUDM/P-tuning) and [P-tuning v2 (ACL'22)](https://github.com/THUDM/P-tuning-v2): pioneer works on ***prompt tuning***
  * [GLM-130B (ICLR'23)](https://github.com/THUDM/GLM-130B): an open bilingual (Enligsh & Chinese) pre-trained model with 130 billion parameters based on [GLM (ACL'22)](https://github.com/THUDM/GLM); better than GPT-3 175B on LAMBADA and MMLU.
  * [ChatGLM-6B](https://github.com/THUDM) & [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) & [ChatGLM3-6B](https://github.com/THUDM/ChatGLM3): an open bilingual dialogue language model that requires only 6GB to run. Receiving [![GitHub stars](https://badgen.net/github/stars/THUDM/ChatGLM-6B)](https://GitHub.com/THUDM/ChatGLM-6B/stargazers/), [![GitHub stars](https://badgen.net/github/stars/THUDM/ChatGLM2-6B)](https://GitHub.com/THUDM/ChatGLM2-6B/stargazers/), and [![GitHub stars](https://badgen.net/github/stars/THUDM/ChatGLM3)](https://GitHub.com/THUDM/ChatGLM3/stargazers/)GitHub Stars!
  * [WebGLM (KDD'23)](https://github.com/THUDM/WebGLM): an efficient web-enhanced question answering system based on [GLM-10B](https://github.com/THUDM/GLM), outperforming WebGPT-13B and approaching WebGPT-175B performance in human evaluation.
  * [ChatGLM-Math](https://arxiv.org/abs/2404.02893): employing *self-critique* with RFT and DPO to enable SOTA mathematical capabilities wihtouth compromising language abilities.
  </details>
  
  <details><summary><b>Foundational Agents For Real-world Challenging Missions</b></summary>
    
  * [AgentBench (ICLR'24)](https://github.com/THUDM/AgentBench): the first systematic multi-dimensional benchmark to *evaluate LLMs as Agents* in 8 distinct environments deriving from real-world practical missions.
  * [AutoWebGLM (KDD'24)](https://github.com/THUDM/AutoWebGLM): a strong web navigating agent constructed upon ChatGLM-3-6B, outperforming prompted GPT-4 on Mind2Web, WebArena, and our constructed new dataset AutoWebBench.
  * [VisualAgentBench](https://github.com/THUDM/VisualAgentBench): a comprehensive framework to train and test Large Multimodal Models (LMMs) to serve as visual foundation agents.
  </details>

  <details><summary><b>Alignment and Scalable Oversights over LLMs and Diffusers</b></summary>
    
  * [ImageReward (NeurIPS'23)](https://github.com/THUDM/ImageReward): the first general-purpose text-to-image human preference reward model (RM) for RLHF, outperforming CLIP/BLIP/Aesthetic by 30% in terms of human preference prediction.
  * [BPO (Black-box Prompt Optimization, ACL'24)](https://github.com/thu-coai/bpo): a novel direction to align LLMs via preference-aware prompt optimization. Improving ChatGPT, Claude, LLaMA on human preference's win rates by 20%+ without training them.
  * [AlignBench (ACL'24)](https://github.com/THUDM/AlignBench): the first comprehensive benchmark on evaluating LLMs' Chinese alignment, deriving from ChatGLM's online real scenarios. 
  </details>
  
  <details><summary><b>Self-supervised Learning and Reasoning</b></summary>

  * [Self-supervised Learning: Generative or Contrastive (TKDE'21)](https://arxiv.org/pdf/2006.08218.pdf): one of the most cited survey on self-supervised learning
  * [SelfKG (WWW'22)](https://github.com/THUDM/SelfKG): self-supervised alignment can be comparable to supervised ones, ***Best Paper Nominee*** in WWW 2022.
  * [kgTransformer (KDD'22)](https://github.com/THUDM/kgTransformer): pre-training knowledge graph transformers with mixture-of-experts (MoE) for complex logical reasoning
  </details>
  
- ðŸ¤” Dedicated to building next-generation of AI systems via both [Large Pre-trained Model](https://github.com/THUDM/GLM-130B) and [Symbolic Agent Reasoning](https://github.com/THUDM/kgTransformer).
- ðŸ’¬ Feel free to drop me an email for:
  * Any form of collaboration
  * Any issue about my works or code
  * Interesting ideas to discuss or just chatting
