### Hi, welcome to my Github ðŸ‘‹

![visitors](https://visitor-badge.glitch.me/badge?page_id=xiao9905.xiao9905&left_color=green&right_color=red)

I am Xiao Liu, a second-year master CS student in Tsinghua University, Knowledge Engineering Group (KEG).

- ðŸ”­ Interested in Machine Learning, Data Mining, NLP and Knowledge Graph.
- ðŸŒ± Find my up-to-date publication list in [Google Scholar](https://scholar.google.com/citations?user=VKI8EhUAAAAJ)! Some of my proud works:
  
  <details><summary><b>Large-scale Language Model Pre-training and Transfer Learning</b></summary>
  
  * [P-tuning](https://github.com/THUDM/P-tuning) and [P-tuning v2](https://github.com/THUDM/P-tuning-v2): pioneer works on ***prompt tuning***
  * [GLM-130B](https://github.com/THUDM/GLM-130B): probably the best open-sourced LLM so far; an open bilingual (Enligsh & Chinese) pre-trained model with 130 billion parameters based on [GLM](https://github.com/THUDM/GLM); better than GPT-3 175B on LAMBADA and MMLU.
  
  </details>
  
  <details><summary><b>Knowledge Graph Construction and Reasoning</b></summary>
  
  * [SelfKG](https://github.com/THUDM/SelfKG): self-supervised alignment can be comparable to supervised ones, ***Best Paper Nominee*** in WWW 2022.
  * [kgTransformer](https://github.com/THUDM/kgTransformer): pre-training knowledge graph transformers for complex logical reasoning
  
  </details>
  
  <details><summary><b>Self-supervised Learning and its Applications</b></summary>
  
  * [Self-supervised Learning: Generative or Contrastive](https://arxiv.org/pdf/2006.08218.pdf): one of the most cited survey on self-supervised learning
  
  </details>
- ðŸ¤” Dedicated to building web-scale knowledge systems via both [Large Pre-trained Model](https://github.com/THUDM/GLM-130B) and [Symbolic Graph Reasoning](https://github.com/THUDM/kgTransformer).
- ðŸ’¬ Feel free to drop me an email for:
  * Any form of collaboration
  * Any issue about my works or code
