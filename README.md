### Hi, welcome to my Github ðŸ‘‹

<!-- ![visitors](https://visitor-badge.glitch.me/badge?page_id=xiao9905.xiao9905&left_color=green&right_color=red) -->

I am Xiao Liu, a third-year PhD student in Tsinghua University since 2021.

- ðŸ”­ Interested in Machine Learning, Data Mining, NLP and Knowledge Graph.
- ðŸŒ± Find my up-to-date publication list in [**Google Scholar**](https://scholar.google.com/citations?user=VKI8EhUAAAAJ)! Some of my proud leading works:
  
  <details><summary><b>Large Language Model (LLM) Training and Prompt Learning</b></summary>
  
  * [P-tuning](https://github.com/THUDM/P-tuning) and [P-tuning v2 (ACL'22)](https://github.com/THUDM/P-tuning-v2): pioneer works on ***prompt tuning***
  * [GLM-130B (ICLR'23)](https://github.com/THUDM/GLM-130B): an open bilingual (Enligsh & Chinese) pre-trained model with 130 billion parameters based on [GLM (ACL'22)](https://github.com/THUDM/GLM); better than GPT-3 175B on LAMBADA and MMLU.
  * [ChatGLM-6B](https://github.com/THUDM) & [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) & [ChatGLM3-6B](https://github.com/THUDM/ChatGLM3): an open bilingual dialogue language model that requires only 6GB to run. Receiving [![GitHub stars](https://badgen.net/github/stars/THUDM/ChatGLM-6B)](https://GitHub.com/THUDM/ChatGLM-6B/stargazers/), [![GitHub stars](https://badgen.net/github/stars/THUDM/ChatGLM2-6B)](https://GitHub.com/THUDM/ChatGLM2-6B/stargazers/), and [![GitHub stars](https://badgen.net/github/stars/THUDM/ChatGLM3)](https://GitHub.com/THUDM/ChatGLM3/stargazers/)GitHub Stars!
  * [WebGLM (KDD'23)](https://github.com/THUDM/WebGLM): an efficient web-enhanced question answering system based on [GLM-10B](https://github.com/THUDM/GLM), outperforming WebGPT-13B and approaching WebGPT-175B performance in human evaluation.
  </details>

  <details><summary><b>Alignment and Scalable Oversights over LLMs</b></summary>

  * [BPO (Black-box Prompt Optimization)](https://github.com/thu-coai/bpo): a novel direction to align LLMs without training them but preference-aware prompt optimization.
  * [AlignBench](https://github.com/THUDM/AlignBench): the first comprehensive benchmark on evaluating LLMs' Chinese alignment, deriving from ChatGLM's online real scenarios.
  * [CritiqueLLM](https://arxiv.org/abs/2311.18702): scaling LLM-as-Critic for scalable oversights on LLM alignment. A series of strong critqiue LLMs ranging from 6B to 66B.
  </details>
  
  <details><summary><b>Evaluation and Assessment of Generative Models (LLMs, Diffusion, ...)</b></summary>
    
  * [ImageReward (NeurIPS'23)](https://github.com/THUDM/ImageReward): the first general-purpose text-to-image human preference reward model (RM) for RLHF, outperforming CLIP/BLIP/Aesthetic by 30% in terms of human preference prediction.
  * [AgentBench](https://github.com/THUDM/AgentBench): the first systematic multi-dimensional benchmark to *evaluate LLMs as Agents* in 8 distinct environments deriving from real-world practical missions. Find LLM-as-Agent demos at [llmbench.ai](https://llmbench.ai)!
  * [LongBench](https://github.com/THUDM/LongBench): a bilingual, multitask benchmark for long context understanding.
  </details>
  
  <details><summary><b>Self-supervised Learning and Reasoning</b></summary>

  * [Self-supervised Learning: Generative or Contrastive (TKDE'21)](https://arxiv.org/pdf/2006.08218.pdf): one of the most cited survey on self-supervised learning
  * [SelfKG (WWW'22)](https://github.com/THUDM/SelfKG): self-supervised alignment can be comparable to supervised ones, ***Best Paper Nominee*** in WWW 2022.
  * [kgTransformer (KDD'22)](https://github.com/THUDM/kgTransformer): pre-training knowledge graph transformers with mixture-of-experts (MoE) for complex logical reasoning
  </details>
  
- ðŸ¤” Dedicated to building next-generation of AI systems via both [Large Pre-trained Model](https://github.com/THUDM/GLM-130B) and [Symbolic Agent Reasoning](https://github.com/THUDM/kgTransformer).
- ðŸ’¬ Feel free to drop me an email for:
  * Any form of collaboration
  * Any issue about my works or code
  * Interesting ideas to discuss or just chatting
